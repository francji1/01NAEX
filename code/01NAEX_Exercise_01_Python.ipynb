{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01NAEX/blob/main/code/01NAEX_Exercise_01_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01NAEX - Exercise 01\n",
        "Data and exercises come from D.C. Montgomery: Design and Analysis of Experiment\n"
      ],
      "metadata": {
        "id": "IJZpZoupsfsX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEGyKc3C8teG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26cEQ8Nt8teK"
      },
      "outputs": [],
      "source": [
        "!pip install rpy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCDVk5ts8teL"
      },
      "outputs": [],
      "source": [
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-tQykDJeaY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, t, f, shapiro\n"
      ],
      "metadata": {
        "id": "AzDtqykGeacb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example from the Lecture"
      ],
      "metadata": {
        "id": "pZMT7l0CRtyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data arrays\n",
        "y1 = np.array([16.85,16.40,17.21,16.35,16.52,17.04,16.96,17.15,16.59,16.57])  # Modified Mortar\n",
        "y2 = np.array([16.62,16.75,17.37,17.12,16.98,16.87,17.34,17.02,17.08,17.27])  # Unmodified Mortar\n",
        "\n",
        "# Sample variances\n",
        "s1_squared = np.var(y1, ddof=1)\n",
        "s2_squared = np.var(y2, ddof=1)\n",
        "\n",
        "# Degrees of freedom\n",
        "dfn = len(y1) - 1  # Degrees of freedom numerator\n",
        "dfd = len(y2) - 1  # Degrees of freedom denominator\n",
        "\n",
        "# F-test statistic\n",
        "F = s1_squared / s2_squared\n",
        "\n",
        "# Two-tailed p-value\n",
        "p_value = 2 * min(f.cdf(F, dfn, dfd), 1 - f.cdf(F, dfn, dfd))\n",
        "\n",
        "print('F-statistic:', F)\n",
        "print('Degrees of freedom:', dfn, 'and', dfd)\n",
        "print('p-value:', p_value)"
      ],
      "metadata": {
        "id": "dDJ8QDnXRtD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Independent two-sample t-test (equal variances)\n",
        "t_statistic, p_value = stats.ttest_ind(y1, y2, equal_var=True)\n",
        "\n",
        "print(f't-statistic: {t_statistic}')\n",
        "print(f'p-value: {p_value}')"
      ],
      "metadata": {
        "id": "Ll7-f7eZRtL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Welch's t-test (unequal variances)\n",
        "t_stat, p_value = stats.ttest_ind(y1, y2, equal_var=False)\n",
        "\n",
        "print('Welch\\'s t-statistic:', t_stat)\n",
        "print('p-value:', p_value)"
      ],
      "metadata": {
        "id": "qi3XwWZeSrWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Two-sample t-test assuming equal variances (var.equal = TRUE in R)\n",
        "t_stat_equal_var, p_value_equal_var = stats.ttest_ind(y1, y2, equal_var=True)\n",
        "\n",
        "# Calculate confidence interval for equal variance\n",
        "n1, n2 = len(y1), len(y2)\n",
        "mean_diff = np.mean(y1) - np.mean(y2)\n",
        "pooled_std = np.sqrt(((n1 - 1) * np.var(y1, ddof=1) + (n2 - 1) * np.var(y2, ddof=1)) / (n1 + n2 - 2))\n",
        "se_pooled = pooled_std * np.sqrt(1/n1 + 1/n2)\n",
        "conf_interval_equal_var = stats.t.interval(0.95, df=n1 + n2 - 2, loc=mean_diff, scale=se_pooled)\n",
        "\n",
        "# 2. Welch's t-test (var.equal = FALSE in R)\n",
        "t_stat_unequal_var, p_value_unequal_var = stats.ttest_ind(y1, y2, equal_var=False)\n",
        "df_unequal_var = ((np.var(y1, ddof=1)/n1 + np.var(y2, ddof=1)/n2)**2) / \\\n",
        "                 ((np.var(y1, ddof=1)/n1)**2/(n1-1) + (np.var(y2, ddof=1)/n2)**2/(n2-1))\n",
        "\n",
        "# Calculate confidence interval for unequal variance\n",
        "se_unequal = np.sqrt(np.var(y1, ddof=1)/n1 + np.var(y2, ddof=1)/n2)\n",
        "conf_interval_unequal_var = stats.t.interval(0.95, df=df_unequal_var, loc=mean_diff, scale=se_unequal)\n",
        "\n",
        "# Results for t-test assuming equal variances\n",
        "print(\"Two-Sample T-Test Assuming Equal Variances\")\n",
        "print(f\"t-statistic: {t_stat_equal_var}\")\n",
        "print(f\"p-value: {p_value_equal_var}\")\n",
        "print(f\"95% confidence interval: {conf_interval_equal_var}\")\n",
        "print(f\"Mean of y1: {np.mean(y1)}, Mean of y2: {np.mean(y2)}\")\n",
        "print()\n",
        "\n",
        "# Results for Welch's t-test (unequal variances)\n",
        "print(\"Welch's Two-Sample T-Test (Assuming Unequal Variances)\")\n",
        "print(f\"t-statistic: {t_stat_unequal_var}\")\n",
        "print(f\"p-value: {p_value_unequal_var}\")\n",
        "print(f\"Degrees of freedom: {df_unequal_var}\")\n",
        "print(f\"95% confidence interval: {conf_interval_unequal_var}\")\n",
        "print(f\"Mean of y1: {np.mean(y1)}, Mean of y2: {np.mean(y2)}\")\n"
      ],
      "metadata": {
        "id": "-uzGYiAwTzDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Kernel Density Plot\n",
        "sns.kdeplot(y1, fill=True, label='Modified Mortar')\n",
        "sns.kdeplot(y2, fill=True, label='Unmodified Mortar')\n",
        "plt.title('Kernel Density Estimation of Mortar Data')\n",
        "plt.xlabel('Tension Bond Strength')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9s-P_osZUzSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QQ-Plot for y1\n",
        "plt.figure()\n",
        "stats.probplot(y1, dist=\"norm\", plot=plt)\n",
        "plt.title('Normal QQ-Plot for Modified Mortar')\n",
        "plt.show()\n",
        "\n",
        "# QQ-Plot for y2\n",
        "plt.figure()\n",
        "stats.probplot(y2, dist=\"norm\", plot=plt)\n",
        "plt.title('Normal QQ-Plot for Unmodified Mortar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FnlUCXkxW7qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapiro-Wilk test for y1\n",
        "statistic_y1, p_value_y1 = shapiro(y1)\n",
        "print(f'Shapiro-Wilk Test for y1: Statistic={statistic_y1}, p-value={p_value_y1}')\n",
        "\n",
        "# Shapiro-Wilk test for y2\n",
        "statistic_y2, p_value_y2 = shapiro(y2)\n",
        "print(f'Shapiro-Wilk Test for y2: Statistic={statistic_y2}, p-value={p_value_y2}')"
      ],
      "metadata": {
        "id": "SJe3JXT4W7tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Parameters\n",
        "effect_size = (np.mean(y1) - np.mean(y2)) / np.sqrt((s1_squared + s2_squared) / 2)\n",
        "alpha = 0.05\n",
        "power = 0.95\n",
        "\n",
        "# Create an instance of the power analysis class\n",
        "analysis = TTestIndPower()\n",
        "\n",
        "# Calculate required sample size\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')\n",
        "print(f'Required sample size per group: {np.ceil(sample_size)}')\n",
        "\n",
        "# Calculate power of the test with n=10\n",
        "actual_power = analysis.power(effect_size=effect_size, nobs1=10, alpha=alpha, alternative='two-sided')\n",
        "print(f'Power of the test with n=10 per group: {actual_power}')"
      ],
      "metadata": {
        "id": "pjc3LPqdXVrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.05\n",
        "power = 0.80\n",
        "sd = 0.284  # Standard deviation\n",
        "effect_sizes = np.array([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6])\n",
        "\n",
        "# Calculate sample sizes\n",
        "analysis = TTestIndPower()\n",
        "sample_sizes = []\n",
        "for delta in effect_sizes:\n",
        "    effect_size = delta / sd\n",
        "    n = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')\n",
        "    sample_sizes.append(n)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(effect_sizes, sample_sizes, marker='o')\n",
        "plt.xlabel('Effect Size')\n",
        "plt.ylabel('Sample Size per Group')\n",
        "plt.title('Sample Size vs. Effect Size')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "owtb-8JNYMhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assigment:\n",
        "\n",
        "* Run and familiarize with Python.\n",
        "* Solve following problems from Montgomery - Design and Analysis of Experiments.\n"
      ],
      "metadata": {
        "id": "7SzDDewJ0Gps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises 2.20\n",
        "\n",
        "The shelf life of a carbonated beverage is of interest. Ten bottles are randomly\n",
        "selected and tested, and the following results are obtained:\n",
        "| Days |     |\n",
        "|------|-----|\n",
        "| 108  | 138 |\n",
        "| 124  | 163 |\n",
        "| 124  | 159 |\n",
        "| 106  | 134 |\n",
        "| 115  | 139 |\n",
        "\n",
        "\n",
        "* We would like to demonstrate that the mean shelf life exceeds 120 days.\n",
        "Set up appropriate hypotheses for investigating this claim.\n",
        "* Test these hypotheses using significant level $\\alpha = 0.01$. Find the P-value\n",
        "for the test. What are your conclusions?\n",
        "* Construct a 99 percent confidence interval on the mean shelf life.\n",
        "* Can shelf life be adequately described or modeled by a normal distribution? What effect would a violation of this assumption have on the test procedure you used in solving previous questions?"
      ],
      "metadata": {
        "id": "zvHL1CwW0VKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the URL\n",
        "url_20 = \"https://raw.githubusercontent.com/francji1/01NAEX/main/data/Ex02_20.csv\"\n",
        "df20 = pd.read_csv(url_20, sep=\";\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df20.head()\n"
      ],
      "metadata": {
        "id": "tQr4UVuXtrC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOLUTION:"
      ],
      "metadata": {
        "id": "FGZPi3stCt3u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MzIG3zv7Qu5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a8QA0oPYtxe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-21\n",
        "\n",
        "In semiconductor manufacturing, wet chemical etching is often used to remove silicon from the backs of wafers prior to metallization. The **etch rate** is an important characteristic of this process. Two different etching solutions are being evaluated. Eight randomly selected wafers have been etched in each solution and the observed etch rates (in mils/min) are shown below:\n",
        "\n",
        "| Solution 1 | Solution 2 |\n",
        "|------------|------------|\n",
        "|  9.9       | 10.2       |\n",
        "|  9.4       | 10.0       |\n",
        "| 10.0       | 10.7       |\n",
        "| 10.3       | 10.5       |\n",
        "| 10.6       | 10.6       |\n",
        "| 10.3       | 10.2       |\n",
        "|  9.3       | 10.4       |\n",
        "|  9.8       | 10.3       |\n",
        "\n",
        "**(a)** Do the data indicate that the claim that both solutions have the same mean etch rate is valid? Use α = 0.05 and assume equal variances.  \n",
        "\n",
        "**(b)** Find a 95% confidence interval on the difference in mean etch rates.  \n",
        "\n",
        "**(c)** Use normal probability plots to investigate the adequacy of the assumptions of normality and equal variances.  \n",
        "\n",
        "**(d)** Compute the power of the test in part (a). If the variance and means corresponds to estimations based on enclosed data, how many measurements per group would be required to achieve power greater than 0.9 to detect a difference of Δ = 0.3 at significance level α = 0.05?\n"
      ],
      "metadata": {
        "id": "sWFOa4jaSqoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the URL\n",
        "url_20 = \"https://raw.githubusercontent.com/francji1/01NAEX/main/data/Ex02_21.csv\"\n",
        "df20 = pd.read_csv(url_20, sep=\";\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df20.head()"
      ],
      "metadata": {
        "id": "wOc_J_dFSsth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:"
      ],
      "metadata": {
        "id": "8RnS5T3dWMuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4klH7vOSx3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_ImOrNbWPgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exercise 2.26\n",
        "\n",
        "The following are the burning times (in minutes) of chemical flares of two different formulations. The design engineers are interested in both the mean and\n",
        "variance of the burning times.\n",
        "\n",
        "|Type1|   | Type2 | |\n",
        "|----|----|----|----|\n",
        "| 65 | 82 | 64 | 56 |\n",
        "| 81 | 67 | 71 | 69 |\n",
        "| 57 | 59 | 83 | 74 |\n",
        "| 66 | 75 | 59 | 82 |\n",
        "| 82 | 70 | 65 | 79 |\n",
        "\n",
        "\n",
        "1. Test the hypothesis that the two variances are equal. Use $\\alpha = 0.05$.\n",
        "2. Using the results of part 1), test the hypothesis that the mean burning\n",
        "times are equal. Use $\\alpha = 0.05$. What is the P-value for this test?\n",
        "3. Discuss the role of the normality assumption in this problem. Check the\n",
        "assumption of normality for both types of flares\n",
        "4. If the mean burning times of the two flares differ by as much as 2 minute, find the power of the test. What sample size would be required to detect an actual difference in mean burning time of 1 minute with a power of at least 0.9?"
      ],
      "metadata": {
        "id": "Ylfv4-d_3SEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the URL\n",
        "url_26 = \"https://raw.githubusercontent.com/francji1/01NAEX/main/data/Ex02_26.csv\"\n",
        "df26 = pd.read_csv(url_26, sep=\";\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df26.head()"
      ],
      "metadata": {
        "id": "YtM7AlkJ0UF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOLUTION:"
      ],
      "metadata": {
        "id": "6Lyi5h_xCq-m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9tpzP9qZq3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VmRp3ESJZq6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLqtiXpaZq9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.30\n",
        "\n",
        "Front housings for cell phones are manufactured in an injection molding process. The time the part is allowed to cool in the mold before removal is thought to influence the occurrence of a particularly troublesome cosmetic defect, flow lines, in the finished housing. After manufacturing, the housings are inspected visually and assigned a score between 1 and 10 based on their appearance, with 10 corresponding to a perfect part and 1 corresponding to a completely defective part. An experiment was conducted using two cool-down times, 10 and 20 seconds, and 20 housings were evaluated at each level of cool-down time. All 40 observations in this experiment were run in random order.\n",
        "\n",
        "\n",
        "||   |   |10s   || |  |  |20s  |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| 1 | 3 | 2 | 6 || 7 | 6 | 8 | 9 |\n",
        "| 1 | 5 | 3 | 3 || 5 | 5 | 9 | 7 |\n",
        "| 5 | 2 | 1 | 1 || 5 | 4 | 8 | 6 |\n",
        "| 5 | 6 | 2 | 8 || 6 | 8 | 4 | 5 |\n",
        "| 3 | 2 | 5 | 3 || 6 | 8 | 7 | 7 |\n",
        "\n",
        "\n",
        "* Is there evidence to support the claim that the longer cool-down time\n",
        "results in fewer appearance defects? Use $\\alpha = 0.05$.\n",
        "* What is the P-value for the test conducted in the previous part?\n",
        "* Find a 95 percent confidence interval on the difference in means. Provide\n",
        "a practical interpretation of this interval.\n",
        "* Compute the power of the test.\n"
      ],
      "metadata": {
        "id": "CU8aDa_V3y_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the URL\n",
        "url_30 = \"https://raw.githubusercontent.com/francji1/01NAEX/main/data/Ex02_30.csv\"\n",
        "df30 = pd.read_csv(url_30, sep=\";\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df30.head()"
      ],
      "metadata": {
        "id": "OPmTG8WruIYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOLUTION:"
      ],
      "metadata": {
        "id": "rvOPQRe_65uE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agLqRAysIp_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L16WhcWlZpp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oe5FPzKdZptE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}