{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01NAEX/blob/main/code/01NAEX_Exercise_01_Python_student_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01NAEX - Exercise 01\n",
        "Data and exercises come from D.C. Montgomery: Design and Analysis of Experiment\n"
      ],
      "metadata": {
        "id": "IJZpZoupsfsX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEGyKc3C8teG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26cEQ8Nt8teK"
      },
      "outputs": [],
      "source": [
        "!pip install rpy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCDVk5ts8teL"
      },
      "outputs": [],
      "source": [
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-tQykDJeaY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, t, f, shapiro\n"
      ],
      "metadata": {
        "id": "AzDtqykGeacb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example from the Lecture"
      ],
      "metadata": {
        "id": "pZMT7l0CRtyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data arrays\n",
        "y1 = np.array([16.85,16.40,17.21,16.35,16.52,17.04,16.96,17.15,16.59,16.57])  # Modified Mortar\n",
        "y2 = np.array([16.62,16.75,17.37,17.12,16.98,16.87,17.34,17.02,17.08,17.27])  # Unmodified Mortar\n",
        "\n",
        "# Sample variances\n",
        "s1_squared = np.var(y1, ddof=1)\n",
        "s2_squared = np.var(y2, ddof=1)\n",
        "\n",
        "# Degrees of freedom\n",
        "dfn = len(y1) - 1  # Degrees of freedom numerator\n",
        "dfd = len(y2) - 1  # Degrees of freedom denominator\n",
        "\n",
        "# F-test statistic\n",
        "F = s1_squared / s2_squared\n",
        "\n",
        "# Two-tailed p-value\n",
        "p_value = 2 * min(f.cdf(F, dfn, dfd), 1 - f.cdf(F, dfn, dfd))\n",
        "\n",
        "print('F-statistic:', F)\n",
        "print('Degrees of freedom:', dfn, 'and', dfd)\n",
        "print('p-value:', p_value)"
      ],
      "metadata": {
        "id": "dDJ8QDnXRtD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Independent two-sample t-test (equal variances)\n",
        "t_statistic, p_value = stats.ttest_ind(y1, y2, equal_var=True)\n",
        "\n",
        "print(f't-statistic: {t_statistic}')\n",
        "print(f'p-value: {p_value}')"
      ],
      "metadata": {
        "id": "Ll7-f7eZRtL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Welch's t-test (unequal variances)\n",
        "t_stat, p_value = stats.ttest_ind(y1, y2, equal_var=False)\n",
        "\n",
        "print('Welch\\'s t-statistic:', t_stat)\n",
        "print('p-value:', p_value)"
      ],
      "metadata": {
        "id": "qi3XwWZeSrWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Two-sample t-test assuming equal variances (var.equal = TRUE in R)\n",
        "t_stat_equal_var, p_value_equal_var = stats.ttest_ind(y1, y2, equal_var=True)\n",
        "\n",
        "# Calculate confidence interval for equal variance\n",
        "n1, n2 = len(y1), len(y2)\n",
        "mean_diff = np.mean(y1) - np.mean(y2)\n",
        "pooled_std = np.sqrt(((n1 - 1) * np.var(y1, ddof=1) + (n2 - 1) * np.var(y2, ddof=1)) / (n1 + n2 - 2))\n",
        "se_pooled = pooled_std * np.sqrt(1/n1 + 1/n2)\n",
        "conf_interval_equal_var = stats.t.interval(0.95, df=n1 + n2 - 2, loc=mean_diff, scale=se_pooled)\n",
        "\n",
        "# 2. Welch's t-test (var.equal = FALSE in R)\n",
        "t_stat_unequal_var, p_value_unequal_var = stats.ttest_ind(y1, y2, equal_var=False)\n",
        "df_unequal_var = ((np.var(y1, ddof=1)/n1 + np.var(y2, ddof=1)/n2)**2) / \\\n",
        "                 ((np.var(y1, ddof=1)/n1)**2/(n1-1) + (np.var(y2, ddof=1)/n2)**2/(n2-1))\n",
        "\n",
        "# Calculate confidence interval for unequal variance\n",
        "se_unequal = np.sqrt(np.var(y1, ddof=1)/n1 + np.var(y2, ddof=1)/n2)\n",
        "conf_interval_unequal_var = stats.t.interval(0.95, df=df_unequal_var, loc=mean_diff, scale=se_unequal)\n",
        "\n",
        "# Results for t-test assuming equal variances\n",
        "print(\"Two-Sample T-Test Assuming Equal Variances\")\n",
        "print(f\"t-statistic: {t_stat_equal_var}\")\n",
        "print(f\"p-value: {p_value_equal_var}\")\n",
        "print(f\"95% confidence interval: {conf_interval_equal_var}\")\n",
        "print(f\"Mean of y1: {np.mean(y1)}, Mean of y2: {np.mean(y2)}\")\n",
        "print()\n",
        "\n",
        "# Results for Welch's t-test (unequal variances)\n",
        "print(\"Welch's Two-Sample T-Test (Assuming Unequal Variances)\")\n",
        "print(f\"t-statistic: {t_stat_unequal_var}\")\n",
        "print(f\"p-value: {p_value_unequal_var}\")\n",
        "print(f\"Degrees of freedom: {df_unequal_var}\")\n",
        "print(f\"95% confidence interval: {conf_interval_unequal_var}\")\n",
        "print(f\"Mean of y1: {np.mean(y1)}, Mean of y2: {np.mean(y2)}\")\n"
      ],
      "metadata": {
        "id": "-uzGYiAwTzDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Kernel Density Plot\n",
        "sns.kdeplot(y1, fill=True, label='Modified Mortar')\n",
        "sns.kdeplot(y2, fill=True, label='Unmodified Mortar')\n",
        "plt.title('Kernel Density Estimation of Mortar Data')\n",
        "plt.xlabel('Tension Bond Strength')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9s-P_osZUzSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QQ-Plot for y1\n",
        "plt.figure()\n",
        "stats.probplot(y1, dist=\"norm\", plot=plt)\n",
        "plt.title('Normal QQ-Plot for Modified Mortar')\n",
        "plt.show()\n",
        "\n",
        "# QQ-Plot for y2\n",
        "plt.figure()\n",
        "stats.probplot(y2, dist=\"norm\", plot=plt)\n",
        "plt.title('Normal QQ-Plot for Unmodified Mortar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FnlUCXkxW7qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapiro-Wilk test for y1\n",
        "statistic_y1, p_value_y1 = shapiro(y1)\n",
        "print(f'Shapiro-Wilk Test for y1: Statistic={statistic_y1}, p-value={p_value_y1}')\n",
        "\n",
        "# Shapiro-Wilk test for y2\n",
        "statistic_y2, p_value_y2 = shapiro(y2)\n",
        "print(f'Shapiro-Wilk Test for y2: Statistic={statistic_y2}, p-value={p_value_y2}')"
      ],
      "metadata": {
        "id": "SJe3JXT4W7tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Parameters\n",
        "effect_size = (np.mean(y1) - np.mean(y2)) / np.sqrt((s1_squared + s2_squared) / 2)\n",
        "alpha = 0.05\n",
        "power = 0.95\n",
        "\n",
        "# Create an instance of the power analysis class\n",
        "analysis = TTestIndPower()\n",
        "\n",
        "# Calculate required sample size\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')\n",
        "print(f'Required sample size per group: {np.ceil(sample_size)}')\n",
        "\n",
        "# Calculate power of the test with n=10\n",
        "actual_power = analysis.power(effect_size=effect_size, nobs1=10, alpha=alpha, alternative='two-sided')\n",
        "print(f'Power of the test with n=10 per group: {actual_power}')"
      ],
      "metadata": {
        "id": "pjc3LPqdXVrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.05\n",
        "power = 0.80\n",
        "sd = 0.284  # Standard deviation\n",
        "effect_sizes = np.array([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6])\n",
        "\n",
        "# Calculate sample sizes\n",
        "analysis = TTestIndPower()\n",
        "sample_sizes = []\n",
        "for delta in effect_sizes:\n",
        "    effect_size = delta / sd\n",
        "    n = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')\n",
        "    sample_sizes.append(n)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(effect_sizes, sample_sizes, marker='o')\n",
        "plt.xlabel('Effect Size')\n",
        "plt.ylabel('Sample Size per Group')\n",
        "plt.title('Sample Size vs. Effect Size')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "owtb-8JNYMhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assigment:\n",
        "\n",
        "* Run and familiarize with Python.\n",
        "* Solve following problems from Montgomery - Design and Analysis of Experiments.\n"
      ],
      "metadata": {
        "id": "7SzDDewJ0Gps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises 2.20\n",
        "\n",
        "The shelf life of a carbonated beverage is of interest. Ten bottles are randomly\n",
        "selected and tested, and the following results are obtained:\n",
        "| Days |     |\n",
        "|------|-----|\n",
        "| 108  | 138 |\n",
        "| 124  | 163 |\n",
        "| 124  | 159 |\n",
        "| 106  | 134 |\n",
        "| 115  | 139 |\n",
        "\n",
        "\n",
        "* We would like to demonstrate that the mean shelf life exceeds 120 days.\n",
        "Set up appropriate hypotheses for investigating this claim.\n",
        "* Test these hypotheses using significant level $\\alpha = 0.01$. Find the P-value\n",
        "for the test. What are your conclusions?\n",
        "* Construct a 99 percent confidence interval on the mean shelf life.\n",
        "* Can shelf life be adequately described or modeled by a normal distribution? What effect would a violation of this assumption have on the test procedure you used in solving previous questions?"
      ],
      "metadata": {
        "id": "zvHL1CwW0VKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the URL\n",
        "url_20 = \"https://raw.githubusercontent.com/francji1/01NAEX/main/data/Ex02_20.csv\"\n",
        "df20 = pd.read_csv(url_20, sep=\";\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df20.head(10)\n"
      ],
      "metadata": {
        "id": "tQr4UVuXtrC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOLUTION:"
      ],
      "metadata": {
        "id": "FGZPi3stCt3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hypotheses setup: H0: μ = 120, H1: μ > 120"
      ],
      "metadata": {
        "id": "CmDUhNMmRCjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = df20['Days'].mean()\n",
        "std = df20['Days'].std()\n",
        "\n",
        "df = len(df20['Days'])-1\n",
        "\n",
        "# test statistic\n",
        "t_stat = (mean - 120) / (std / np.sqrt(10))\n",
        "\n",
        "#to reject the null hypothesis, the estimated mean has to be greater than 120 (so the statistic > 0) and the p-value of the test statistic needs to be lower than 0.01 (we do only one-sided test)\n",
        "p_val = t.sf(np.abs(t_stat), df)\n",
        "print(f't_stat: {t_stat}, p_val: {p_val}')\n"
      ],
      "metadata": {
        "id": "MzIG3zv7Qu5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose normality of the distribution of shelf life, so $shelf \\text{ }life ∼ N(μ , σ)$. In gerenal, the unbiased estimate of sample mean of random variable $X$ is $\\hat{\\mu} = \\frac{1}{n}\\sum_{i = 1}^{n}X_i$, where $X_i$ $iid$ replications of $X$. In our case,  $X \\sim N(μ , σ)$, thus, from the reproductive property of normal distribution we obtain $\\hat{\\mu} \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}})$. Of course, when $\\sigma$ is not known, one needs to use the unbiased estimate $\\hat{\\sigma}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "jWjjMOb_TcpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, one might argue that shelf life is non-negative, thus cannot be modelled as a random variable with normal distribution. As long as shelf life is $\\mathcal{L}_2$ random variable and the replications are $iid$, we can rely on the central limit theorem, according to which $\\hat{\\mu}$ is at least asymptotically normal. One thing to consider is that 10 observations might not be enough to guarantee that the distribution of $\\hat{\\mu}$ is sufficiently close to normal."
      ],
      "metadata": {
        "id": "Pc2jaH3mO5Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantile_995 = norm.ppf(0.995, loc=0, scale=1)\n",
        "\n",
        "print(f'the 99% [{mean - std / np.sqrt(10)*quantile_995}, {mean + std / np.sqrt(10)*quantile_995}]')"
      ],
      "metadata": {
        "id": "a8QA0oPYtxe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-21\n",
        "\n",
        "In semiconductor manufacturing, wet chemical etching is often used to remove silicon from the backs of wafers prior to metallization. The **etch rate** is an important characteristic of this process. Two different etching solutions are being evaluated. Eight randomly selected wafers have been etched in each solution and the observed etch rates (in mils/min) are shown below:\n",
        "\n",
        "| Solution 1 | Solution 2 |\n",
        "|------------|------------|\n",
        "|  9.9       | 10.2       |\n",
        "|  9.4       | 10.0       |\n",
        "| 10.0       | 10.7       |\n",
        "| 10.3       | 10.5       |\n",
        "| 10.6       | 10.6       |\n",
        "| 10.3       | 10.2       |\n",
        "|  9.3       | 10.4       |\n",
        "|  9.8       | 10.3       |\n",
        "\n",
        "**(a)** Do the data indicate that the claim that both solutions have the same mean etch rate is valid? Use α = 0.05 and assume equal variances.  \n",
        "\n",
        "**(b)** Find a 95% confidence interval on the difference in mean etch rates.  \n",
        "\n",
        "**(c)** Use normal probability plots to investigate the adequacy of the assumptions of normality and equal variances.  \n",
        "\n",
        "**(d)** Compute the power of the test in part (a). If the variance and means corresponds to estimations based on enclosed data, how many measurements per group would be required to achieve power greater than 0.9 to detect a difference of Δ = 0.3 at significance level α = 0.05?\n"
      ],
      "metadata": {
        "id": "sWFOa4jaSqoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the URL\n",
        "url_20 = \"https://raw.githubusercontent.com/francji1/01NAEX/main/data/Ex02_21.csv\"\n",
        "df20 = pd.read_csv(url_20, sep=\";\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df20.head()"
      ],
      "metadata": {
        "id": "wOc_J_dFSsth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:"
      ],
      "metadata": {
        "id": "8RnS5T3dWMuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_1 = df20['Solution1'].mean()\n",
        "mean_2 = df20['Solution2'].mean()\n",
        "std_1 = df20['Solution1'].std()\n",
        "std_2 = df20['Solution2'].std()\n",
        "\n",
        "pooled_std = np.sqrt(((len(df20['Solution1'])-1) * std_1**2 + (len(df20['Solution2'])-1) * std_2**2) / (len(df20['Solution1'])+len(df20['Solution2'])-2))\n",
        "\n",
        "# test statistic\n",
        "t_stat = (mean_1 - mean_2) / (pooled_std * np.sqrt(1/len(df20['Solution1']) + 1/len(df20['Solution2'])))\n",
        "\n",
        "# we can answer question a) and b) at the same time since rejecting null hypothesis on 0.05 confidence level is equivalent to the fact that 0 does not lie within the 95% confidence interval\n",
        "b_lower = mean_1 - mean_2 - t.ppf(0.975, len(df20['Solution1'])+len(df20['Solution2'])-2) * pooled_std * np.sqrt(1/len(df20['Solution1']) + 1/len(df20['Solution2']))\n",
        "b_upper = mean_1 - mean_2 + t.ppf(0.975, len(df20['Solution1'])+len(df20['Solution2'])-2) * pooled_std * np.sqrt(1/len(df20['Solution1']) + 1/len(df20['Solution2']))\n",
        "\n",
        "print(f't_stat: {t_stat}, b_lower: {b_lower}, b_upper: {b_upper}')\n"
      ],
      "metadata": {
        "id": "x4klH7vOSx3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QQ-Plot for Solution1\n",
        "plt.figure()\n",
        "stats.probplot(y1, dist=\"norm\", plot=plt)\n",
        "plt.title('Normal QQ-Plot for first Solution')\n",
        "plt.show()\n",
        "\n",
        "# QQ-Plot for Solution2\n",
        "plt.figure()\n",
        "stats.probplot(y2, dist=\"norm\", plot=plt)\n",
        "plt.title('Normal QQ-Plot for second Solution')\n",
        "plt.show()\n",
        "\n",
        "from statsmodels.graphics.gofplots import qqplot_2samples\n",
        "\n",
        "# QQ-Plot to assess equal variance\n",
        "plt.figure()\n",
        "qqplot_2samples(df20['Solution1']- mean_1, df20['Solution2'] - mean_2, xlabel='Solution1', ylabel='Solution2', line='45')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U_ImOrNbWPgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Size of test: the probability, that, given that two means are different, the random variable from the non-central student distribution falls into the acceptance region of the null hypothesis"
      ],
      "metadata": {
        "id": "NrwjfmH1XZIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import nct\n",
        "def power_two_sample_t(n, delta, sigma, alpha):\n",
        "    df = 2*n - 2\n",
        "    # noncentrality parameter\n",
        "    lambda_ = delta / (sigma * np.sqrt(2/n))\n",
        "    # critical value for two-sided test\n",
        "    t_crit = t.ppf(1 - alpha/2, df)\n",
        "    # power calculation using noncentral t CDF\n",
        "    power = nct.cdf(-t_crit, df, lambda_) + (1 - nct.cdf(t_crit, df, lambda_))\n",
        "    return power\n",
        "\n",
        "calc_power = power_two_sample_t(len(df20['Solution1']), abs(mean_1-mean_2), pooled_std, 0.05)\n",
        "print(f'power of the test: {calc_power}')"
      ],
      "metadata": {
        "id": "DZ1_2-sQSeKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the desired number of observations not using normal approximations, one can use iterative approach:"
      ],
      "metadata": {
        "id": "5W1PRjl9aIZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# search for minimum n\n",
        "n = 2\n",
        "while True:\n",
        "    pwr = power_two_sample_t(n, 0.3, pooled_std, 0.05)\n",
        "    if pwr >= 0.9:\n",
        "        break\n",
        "    n += 1\n",
        "\n",
        "print(f\"Required sample size per group: {n}\")\n",
        "print(f\"Achieved power: {pwr:.4f}\")"
      ],
      "metadata": {
        "id": "KsSDJZd5aXQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exercise 2.26\n",
        "\n",
        "The following are the burning times (in minutes) of chemical flares of two different formulations. The design engineers are interested in both the mean and\n",
        "variance of the burning times.\n",
        "\n",
        "|Type1|   | Type2 | |\n",
        "|----|----|----|----|\n",
        "| 65 | 82 | 64 | 56 |\n",
        "| 81 | 67 | 71 | 69 |\n",
        "| 57 | 59 | 83 | 74 |\n",
        "| 66 | 75 | 59 | 82 |\n",
        "| 82 | 70 | 65 | 79 |\n",
        "\n",
        "\n",
        "1. Test the hypothesis that the two variances are equal. Use $\\alpha = 0.05$.\n",
        "2. Using the results of part 1), test the hypothesis that the mean burning\n",
        "times are equal. Use $\\alpha = 0.05$. What is the P-value for this test?\n",
        "3. Discuss the role of the normality assumption in this problem. Check the\n",
        "assumption of normality for both types of flares\n",
        "4. If the mean burning times of the two flares differ by as much as 2 minute, find the power of the test. What sample size would be required to detect an actual difference in mean burning time of 1 minute with a power of at least 0.9?"
      ],
      "metadata": {
        "id": "Ylfv4-d_3SEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the URL\n",
        "url_26 = \"https://raw.githubusercontent.com/francji1/01NAEX/main/data/Ex02_26.csv\"\n",
        "df26 = pd.read_csv(url_26, sep=\";\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df26.head()"
      ],
      "metadata": {
        "id": "YtM7AlkJ0UF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOLUTION:"
      ],
      "metadata": {
        "id": "6Lyi5h_xCq-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two possibilities of testing equality of variance are presented here, the first one is via classical F-test:"
      ],
      "metadata": {
        "id": "h5Yd4OFu1_Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_1 = df26['Type1'].var()\n",
        "var_2 = df26['Type2'].var()\n",
        "\n",
        "print(f'var_1: {var_1}, var_2: {var_2}')"
      ],
      "metadata": {
        "id": "h9tpzP9qZq3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F_stat = var_2 / var_1\n",
        "df1 = len(df26['Type1'])-1\n",
        "df2 = len(df26['Type2'])-1\n",
        "p_val = f.sf(F_stat, df1, df2)\n",
        "\n",
        "print(f'p_val: {p_val}')"
      ],
      "metadata": {
        "id": "VmRp3ESJZq6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The downside of F-test is its reliance on normality of data. The theoretical foundation of F-test relies in the fact that $\\frac{\\hat{\\sigma}^2}{σ} = \\frac{1}{σ(n-1)}∑_1^n(X_i - \\hat{\\mu})^2 ∼ χ^2(n-1)$, which is only true when $X$ is normally distributed and the sample variables $X_i$ are $iid$. Note that central limit theorem does not help in this case."
      ],
      "metadata": {
        "id": "vPolDlSX2hTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_1 = df26['Type1'].mean()\n",
        "mean_2 = df26['Type2'].mean()\n",
        "\n",
        "pooled_std = np.sqrt(((len(df26['Type1'])-1) * var_1 + (len(df26['Type2'])-1) * var_2) / (len(df26['Type1'])+len(df26['Type2'])-2))\n",
        "\n",
        "# test statistic\n",
        "t_stat = (mean_1 - mean_2) / (pooled_std * np.sqrt(1/len(df26['Type1']) + 1/len(df26['Type2'])))\n",
        "p_value = 2 * min(t.cdf(t_stat, len(df26['Type1'])+len(df26['Type2'])-2), 1 - t.cdf(t_stat, len(df26['Type1'])+len(df26['Type2'])-2))\n",
        "print(f't_stat: {t_stat}, p_value: {p_value}')\n"
      ],
      "metadata": {
        "id": "YLqtiXpaZq9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us check the assumption of normality via Shapiro-Wilk test\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "stat_1, p_value_1 = shapiro(df26['Type1'])\n",
        "stat_2, p_value_2 = shapiro(df26['Type2'])\n",
        "\n",
        "print(f\"Shapiro-Wilk test for Type1 \\n\")\n",
        "print(f\"  W statistic = {stat_1:.4f}\")\n",
        "print(f\"  p-value = {p_value_1:.4f}\\n\")\n",
        "print(f'Shapiro-Wilk test for Type2 \\n')\n",
        "print(f\"  W statistic = {stat_2:.4f}\")\n",
        "print(f\"  p-value = {p_value_2:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YbIc0QvJ97Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to calculate power of the test, we can use the same function as in the previous exercise\n",
        "calc_power = power_two_sample_t(len(df26['Type1']), 2, pooled_std, 0.05)\n",
        "print(f'power of the test: {calc_power}')"
      ],
      "metadata": {
        "id": "ZwOgQn2s_vQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search for minimum n\n",
        "n = 2\n",
        "while True:\n",
        "    pwr = power_two_sample_t(n, 1, pooled_std, 0.05)\n",
        "    if pwr >= 0.9:\n",
        "        break\n",
        "    n += 1\n",
        "\n",
        "print(f\"Required sample size per group: {n}\")\n",
        "print(f\"Achieved power: {pwr:.4f}\")"
      ],
      "metadata": {
        "id": "PJsyQdeTBWgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.30\n",
        "\n",
        "Front housings for cell phones are manufactured in an injection molding process. The time the part is allowed to cool in the mold before removal is thought to influence the occurrence of a particularly troublesome cosmetic defect, flow lines, in the finished housing. After manufacturing, the housings are inspected visually and assigned a score between 1 and 10 based on their appearance, with 10 corresponding to a perfect part and 1 corresponding to a completely defective part. An experiment was conducted using two cool-down times, 10 and 20 seconds, and 20 housings were evaluated at each level of cool-down time. All 40 observations in this experiment were run in random order.\n",
        "\n",
        "\n",
        "||   |   |10s   || |  |  |20s  |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| 1 | 3 | 2 | 6 || 7 | 6 | 8 | 9 |\n",
        "| 1 | 5 | 3 | 3 || 5 | 5 | 9 | 7 |\n",
        "| 5 | 2 | 1 | 1 || 5 | 4 | 8 | 6 |\n",
        "| 5 | 6 | 2 | 8 || 6 | 8 | 4 | 5 |\n",
        "| 3 | 2 | 5 | 3 || 6 | 8 | 7 | 7 |\n",
        "\n",
        "\n",
        "* Is there evidence to support the claim that the longer cool-down time\n",
        "results in fewer appearance defects? Use $\\alpha = 0.05$.\n",
        "* What is the P-value for the test conducted in the previous part?\n",
        "* Find a 95 percent confidence interval on the difference in means. Provide\n",
        "a practical interpretation of this interval.\n",
        "* Compute the power of the test.\n"
      ],
      "metadata": {
        "id": "CU8aDa_V3y_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the URL\n",
        "url_30 = \"https://raw.githubusercontent.com/francji1/01NAEX/main/data/Ex02_30.csv\"\n",
        "df30 = pd.read_csv(url_30, sep=\";\")\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df30.head()"
      ],
      "metadata": {
        "id": "OPmTG8WruIYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOLUTION:"
      ],
      "metadata": {
        "id": "rvOPQRe_65uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import levene\n",
        "df30.columns = ['10s', '20s']\n",
        "mean_10 = df30['10s'].mean()\n",
        "mean_20 = df30['20s'].mean()\n",
        "std_10 = df30['10s'].std()\n",
        "std_20 = df30['20s'].std()\n",
        "\n",
        "# at first, assess the equivalence of variances\n",
        "\n",
        "stat, p_value_levene = levene(df30['10s'], df30['20s'], center='median')\n",
        "\n",
        "print(\"\\nLevene’s test for equality of variance\")\n",
        "print(f\"  W statistic = {stat:.4f}, p-value = {p_value_levene:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "agLqRAysIp_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the test showed that there is not enough evidence that the variances are different, so lets use the assumption of equal variances\n",
        "pooled_std = np.sqrt(((len(df30['10s'])-1) * std_10**2 + (len(df30['20s'])-1) * std_20**2) / (len(df30['10s'])+len(df30['20s'])-2))\n",
        "\n",
        "# test statistic (remember - the higher the mean, the less appearance defects so we need to check whether the statistic is sufficiently above zero)\n",
        "t_stat = (mean_20 - mean_10) / (pooled_std * np.sqrt(1/len(df30['10s']) + 1/len(df30['20s'])))\n",
        "# p-value\n",
        "p_value = 1-t.cdf(t_stat, len(df30['10s'])+len(df30['20s'])-2)\n",
        "print(f't_stat: {t_stat}, p_value: {p_value}')"
      ],
      "metadata": {
        "id": "c3coFscNLoV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now find the 0.95 confidence interval:\n",
        "\n",
        "b_lower = mean_20 - mean_10 - (t.ppf(0.975, len(df30['10s'])+len(df30['20s'])-2) * pooled_std * np.sqrt(1/len(df30['10s']) + 1/len(df30['20s'])))\n",
        "b_upper = mean_20 - mean_10 + (t.ppf(0.975, len(df30['10s'])+len(df30['20s'])-2) * pooled_std*np.sqrt(1/len(df30['10s']) + 1/len(df30['20s'])))\n",
        "\n",
        "print(f'b_lower: {b_lower}, b_upper: {b_upper}')"
      ],
      "metadata": {
        "id": "L16WhcWlZpp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**95 percent confidence interval**: true difference between means lies within borders of this interval with probability of 0.95"
      ],
      "metadata": {
        "id": "4nKIiinx67OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "power_two_sample_t(len(df30['10s']), abs(mean_20-mean_10), pooled_std, 0.05)"
      ],
      "metadata": {
        "id": "Oe5FPzKdZptE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}